# Deep Implicit Attention
Collection of experimental PyTorch implementations of <a href="https://implicit-layers-tutorial.org/">deep implicit layers</a> for self-attention.

Playing around with ideas explained in a series of blog posts:
- <a href="https://mcbal.github.io/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/">Transformer Attention as an Implicit Mixture of Effective Energy-Based Models</a> (December 24th 2020)
- <a href="https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/">An Energy-Based Perspective on Attention Mechanisms in Transformers</a> (November 28th 2020)

## Todo

- [ ] Figure out whether it's possible to recast vanilla `softmax` attention as a viable deep implicit layer
